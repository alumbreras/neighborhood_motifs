# cast dates to numeric
dates <- as.numeric(V(gp)$date)
gp <- remove.vertex.attribute(gp, "date")
V(gp)$date <- dates
rad <- 4
max.neighbors <- 20
j <- 1
#breakpoints <- changepoints(gp, vertical=T, horizontal=T)
#breakpoints <- changepoints.bertrand(gp, vertical=T, horizontal=T)
breakpoints <- changepoints.cp(gp, vertical=T, horizontal=T)
breakpoints.h <- breakpoints$breakpoints.h
breakpoints.v <- breakpoints$breakpoints.v
breakpoints.vh <- breakpoints$breakpoints.vh
# Detect and plot neighborhoods
#for(i in 1:vcount(gp)){
#  eg <- neighborhood.temporal(gp, i, 3, breakpoints.v, breakpoints.h)
#  plot(eg)
#}
par(mfrow=c(1,1))
gp_ <- as.undirected(gp)
la = layout_as_tree(gp_, mode='out', root=which.min(V(gp_)$date))
V(gp_)$size <- 1
V(gp_)$color <- 'black'
#V(gp_)[V(eg)$name]$color <- 'darkgreen'
#V(gp_)[V(eg)$name]$size <- 2
V(gp_)[j]$size <- 3
V(gp_)[j]$color <- 'green'
V(gp_)[breakpoints.v]$color <- 'orange'
V(gp_)[breakpoints.h]$color <- 'yellow'
V(gp_)[breakpoints.vh]$color <- 'red'
plot(gp_,
layout = la,
#vertex.label = order(V(gp)),
vertex.label = NA,
edge.width = 0.2,
edge.arrow.size=0.02,
asp=9/16,
margin=-0.15)
posts <- V(gp)[which(ego_size(gp, 2, mode='out', mindist = 1)==1)]
dates <- sort(posts$date)
res <- cpt.meanvar(dates, class=TRUE, method="PELT")
#res <- cpt.meanvar(dates, penalty="None", class=TRUE, method="SegNeigh") # slow
plot(res)
bp <- cpts(res)+1
colors <- rep("black", length(dates))
colors[bp] <- 'red'
plot(dates, col=colors, pch=19, cex=0.5)
la = layout_as_tree(gp_, mode='out', root=which.min(V(gp_)$date), circular=TRUE)
V(gp_)$size <- 1
V(gp_)$color <- 'black'
#V(gp_)[V(eg)$name]$color <- 'darkgreen'
#V(gp_)[V(eg)$name]$size <- 2
V(gp_)[j]$size <- 3
V(gp_)[j]$color <- 'green'
V(gp_)[breakpoints.v]$color <- 'orange'
V(gp_)[breakpoints.h]$color <- 'yellow'
V(gp_)[breakpoints.vh]$color <- 'red'
plot(gp_,
layout = la,
#vertex.label = order(V(gp)),
vertex.label = NA,
edge.width = 0.2,
edge.arrow.size=0.02,
asp=9/16,
margin=-0.15)
par(mfrow=c(1,1))
gp_ <- as.undirected(gp)
la = layout_as_tree(gp_, mode='out', root=which.min(V(gp_)$date), circular=FALSE)
V(gp_)$size <- 1
V(gp_)$color <- 'black'
#V(gp_)[V(eg)$name]$color <- 'darkgreen'
#V(gp_)[V(eg)$name]$size <- 2
V(gp_)[j]$size <- 3
V(gp_)[j]$color <- 'green'
V(gp_)[breakpoints.v]$color <- 'orange'
V(gp_)[breakpoints.h]$color <- 'yellow'
V(gp_)[breakpoints.vh]$color <- 'red'
plot(gp_,
layout = la,
#vertex.label = order(V(gp)),
vertex.label = NA,
edge.width = 0.2,
edge.arrow.size=0.02,
asp=9/16,
margin=-0.15)
# Horizontal
posts <- V(gp)[which(ego_size(gp, 2, mode='out', mindist = 1)==1)]
dates <- sort(posts$date)
res <- cpt.meanvar(dates, class=TRUE, method="PELT")
#res <- cpt.meanvar(dates, penalty="None", class=TRUE, method="SegNeigh") # slow
plot(res)
bp <- cpts(res)+1
colors <- rep("black", length(dates))
colors[bp] <- 'red'
plot(dates, col=colors, pch=19, cex=0.5)
children
source('~/Documentos/PhD/src/neighborhood_motifs/R/tree_changepoints.r')
rad <- 4
max.neighbors <- 20
j <- 1
#breakpoints <- changepoints(gp, vertical=T, horizontal=T)
#breakpoints <- changepoints.bertrand(gp, vertical=T, horizontal=T)
breakpoints <- changepoints.cp(gp, vertical=T, horizontal=T)
breakpoints.h <- breakpoints$breakpoints.h
breakpoints.v <- breakpoints$breakpoints.v
breakpoints.vh <- breakpoints$breakpoints.vh
# Detect and plot neighborhoods
#for(i in 1:vcount(gp)){
#  eg <- neighborhood.temporal(gp, i, 3, breakpoints.v, breakpoints.h)
#  plot(eg)
#}
par(mfrow=c(1,1))
gp_ <- as.undirected(gp)
la = layout_as_tree(gp_, mode='out', root=which.min(V(gp_)$date), circular=FALSE)
V(gp_)$size <- 1
V(gp_)$color <- 'black'
#V(gp_)[V(eg)$name]$color <- 'darkgreen'
#V(gp_)[V(eg)$name]$size <- 2
V(gp_)[j]$size <- 3
V(gp_)[j]$color <- 'green'
V(gp_)[breakpoints.v]$color <- 'orange'
V(gp_)[breakpoints.h]$color <- 'yellow'
V(gp_)[breakpoints.vh]$color <- 'red'
plot(gp_,
layout = la,
#vertex.label = order(V(gp)),
vertex.label = NA,
edge.width = 0.2,
edge.arrow.size=0.02,
asp=9/16,
margin=-0.15)
source('~/Documentos/PhD/src/neighborhood_motifs/R/tree_changepoints.r')
setwd("~/Documentos/PhD/src/neighborhood_motifs")
# Extract list of triads where a user participates.
#
# author: Alberto Lumbreras
#
#https://www.reddit.com/r/europe/CasualConversation
#https://www.reddit.com/r/europe/
#https://www.reddit.com/r/datascience
#https://www.reddit.com/r/science/
#https://www.reddit.com/r/france
#https://www.reddit.com/r/catalunya
#https://www.reddit.com/r/es
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
setwd("~/Documentos/PhD/src/neighborhood_motifs")
# Extract list of triads where a user participates.
#
# author: Alberto Lumbreras
#
#https://www.reddit.com/r/europe/CasualConversation
#https://www.reddit.com/r/europe/
#https://www.reddit.com/r/datascience
#https://www.reddit.com/r/science/
#https://www.reddit.com/r/france
#https://www.reddit.com/r/catalunya
#https://www.reddit.com/r/es
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
source('R/load_participations.r')
source('R/count_motifs.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/tree_changepoints.r')
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
source('R/load_participations.r')
source('R/count_motifs.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
source('~/Documentos/PhD/src/neighborhood_motifs/pipeline_clustering.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
source('R/count_motifs.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
MIN_POSTS <- 100 # number of post to consider a user as active
###################################################
# Load data
###################################################
#df.posts <- load_posts(database='reddit', forum='podemos')
#save(df.posts,file="dfposts.Rda")
load('dfposts.Rda') # 836119 posts, 47803 threads
df.posts <- data.frame(df.posts) %>% arrange(date)
df.posts <- df.posts[1:300000,]
df.posts <- df.posts[1:100000,]
df.threads <- plyr::count(df.posts, "thread")
df.users <- plyr::count(df.posts, 'user')
names(df.threads)[2] <- "length"
names(df.users)[2] <- "posts"
start.date <- as.POSIXct(min(as.numeric(df.posts$date)), origin = "1970-01-01")
end.date <- as.POSIXct(max(as.numeric(df.posts$date)), origin = "1970-01-01")
print(paste("Start date:", start.date))
print(paste("End date:", end.date))
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/1000))
length(chunks)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
registerDoParallel(cl)
pck <- c('RSQLite', 'data.table')
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='order')
}
stopCluster(cl)
save(res,file="res_time.Rda")
plot.motif.counts(res)
res <- merge.motif.counts(res.parallel)
plot.motif.counts(res)
source('~/Documentos/PhD/src/neighborhood_motifs/R/count_motifs.r')
plot.motif.counts(res)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
registerDoParallel(cl)
pck <- c('RSQLite', 'data.table')
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='time')
}
stopCluster(cl)
traceback()
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
registerDoParallel(cl)
pck <- c('RSQLite', 'data.table', 'changepoint')
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='time')
}
stopCluster(cl)
res.parallel
res <- merge.motif.counts(res.parallel)
save(res,file="res_time.Rda")
plot.motif.counts(res)
stopCluster(cl)
res <- merge.motif.counts(res.parallel)
#save(res,file="res_3_4_order.Rda")
save(res,file="res_time.Rda")
plot.motif.counts(res)
setwd("~/Documentos/PhD/src/neighborhood_motifs")
# Extract list of triads where a user participates.
#
# author: Alberto Lumbreras
#
#https://www.reddit.com/r/europe/CasualConversation
#https://www.reddit.com/r/europe/
#https://www.reddit.com/r/datascience
#https://www.reddit.com/r/science/
#https://www.reddit.com/r/france
#https://www.reddit.com/r/catalunya
#https://www.reddit.com/r/es
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
MIN_POSTS <- 100 # number of post to consider a user as active
###################################################
# Load data
###################################################
#df.posts <- load_posts(database='reddit', forum='podemos')
#save(df.posts,file="dfposts.Rda")
load('dfposts.Rda') # 836119 posts, 47803 threads
df.posts <- data.frame(df.posts) %>% arrange(date)
df.posts <- df.posts[1:300000,]
df.posts <- df.posts[1:100000,]
df.posts <- df.posts[1:1000,] # test debug
df.threads <- plyr::count(df.posts, "thread")
df.users <- plyr::count(df.posts, 'user')
names(df.threads)[2] <- "length"
names(df.users)[2] <- "posts"
# Compute neighborhood around every post
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/1000))
length(chunks)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
registerDoParallel(cl)
pck <- c('RSQLite', 'data.table', 'changepoint')
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='time')
}
stopCluster(cl)
res.seq <- count_motifs_by_post(as.vector(unlist(chunks[1])),
database='reddit',
neighbourhood='order')
res.seq.dyn <- count_motifs_by_post(as.vector(unlist(chunks[1])),
database='reddit',
neighbourhood='time')
setwd("~/Documentos/PhD/src/neighborhood_motifs")
# Extract list of triads where a user participates.
#
# author: Alberto Lumbreras
#
#https://www.reddit.com/r/europe/CasualConversation
#https://www.reddit.com/r/europe/
#https://www.reddit.com/r/datascience
#https://www.reddit.com/r/science/
#https://www.reddit.com/r/france
#https://www.reddit.com/r/catalunya
#https://www.reddit.com/r/es
library(parallel)
library(doParallel)
library(foreach)
library(ggplot2)
library(ggbiplot)
library(gplots)
library(dplyr)
library(reshape2)
library(igraph)
library(ggbiplot)
#library(RSQLite)
#library(GGally)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
MIN_POSTS <- 100 # number of post to consider a user as active
###################################################
# Load data
###################################################
#df.posts <- load_posts(database='reddit', forum='podemos')
#save(df.posts,file="dfposts.Rda")
load('dfposts.Rda') # 836119 posts, 47803 threads
df.posts <- data.frame(df.posts) %>% arrange(date)
df.posts <- df.posts[1:300000,]
df.posts <- df.posts[1:100000,]
df.posts <- df.posts[1:100,] # test debug
df.threads <- plyr::count(df.posts, "thread")
df.users <- plyr::count(df.posts, 'user')
names(df.threads)[2] <- "length"
names(df.users)[2] <- "posts"
# Compute neighborhood around every post
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/1000))
length(chunks)
res.seq.dyn <- count_motifs_by_post(as.vector(unlist(chunks[1])),
database='reddit',
neighbourhood='time')
res <- res.seq.dyn
plot.motif.counts(res)
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='time')
}
stopCluster(cl)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
registerDoParallel(cl)
pck <- c('RSQLite', 'data.table', 'changepoint')
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='time')
}
stopCluster(cl)
res <- merge.motif.counts(res.parallel)
res.parallel
res.parallel[[1]]
res.parallel[[2]]
res.parallel[[1]]
res <- res.parallel[[1]]
plot.motif.counts(res)
source('R/load_participations.r')
source('R/count_motifs.r')
source('R/normalize_counts.r')
source('R/clustering.r')
MIN_POSTS <- 100 # number of post to consider a user as active
###################################################
# Load data
###################################################
#df.posts <- load_posts(database='reddit', forum='podemos')
#save(df.posts,file="dfposts.Rda")
load('dfposts.Rda') # 836119 posts, 47803 threads
df.posts <- data.frame(df.posts) %>% arrange(date)
df.posts <- df.posts[1:300000,]
df.posts <- df.posts[1:100000,]
df.posts <- df.posts[1:1000,] # test debug
df.threads <- plyr::count(df.posts, "thread")
df.users <- plyr::count(df.posts, 'user')
names(df.threads)[2] <- "length"
names(df.users)[2] <- "posts"
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/1000))
length(chunks)
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/500))
length(chunks)
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/200))
length(chunks)
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/20))
length(chunks)
chunks <- split(df.threads$thread, ceiling(seq_along(df.threads$thread)/100))
length(chunks)
ncores <- detectCores() - 2
cl<-makeCluster(ncores, outfile="", port=11439)
registerDoParallel(cl)
pck <- c('RSQLite', 'data.table', 'changepoint')
res.parallel <- foreach(i=1:length(chunks), .packages = pck)%dopar%{
source('R/extract_from_db.r')
count_motifs_by_post(chunks[[i]],
database='reddit',
neighbourhood='time')
}
res.parallel
